syntax = "proto3";
package influxdata.iox.management.v1;
option go_package = "github.com/influxdata/iox/management/v1";

message OperationMetadata {
  // How many nanoseconds of CPU time have been spent on this job so far?
  uint64 cpu_nanos = 1;

  // How many nanoseconds has it been since the job was submitted
  uint64 wall_nanos = 2;

  // The total number of created tasks
  uint64 total_count = 3;

  // The number of pending tasks
  uint64 pending_count = 4;

  // The number of tasks that completed successfully
  uint64 success_count = 13;

  // The number of tasks that returned an error
  uint64 error_count = 14;

  // The number of tasks that were cancelled
  uint64 cancelled_count = 15;

  // The number of tasks that did not run to completion (e.g. panic)
  uint64 dropped_count = 16;

  // What kind of job is it?
  oneof job {
    Dummy dummy = 5;
    /* historical artifact
    PersistSegment persist_segment = 6;
    */
    CloseChunk close_chunk = 7;
    WriteChunk write_chunk = 8;
    WipePreservedCatalog wipe_preserved_catalog = 9;
    CompactChunks compact_chunks = 10;
    PersistChunks persist_chunks = 11;
    DropChunk drop_chunk = 12;
    DropPartition drop_partition = 17;
  }
}

// A job that simply sleeps for a specified time and then returns success
message Dummy {
  // How long the job should sleep for before returning
  repeated uint64 nanos = 1;

  // Name of the database, if any
  string db_name = 2;
}

// Move a chunk from mutable buffer to read buffer
message CloseChunk {
  // name of the database
  string db_name = 1;

  // partition key
  string partition_key = 2;

  // table name
  string table_name = 4;

  // chunk_id
  uint32 chunk_id = 3;
}

// Write a chunk from read buffer to object store
message WriteChunk {
  // name of the database
  string db_name = 1;

  // partition key
  string partition_key = 2;

  // table name
  string table_name = 4;

  // chunk_id
  uint32 chunk_id = 3;
}

// Compact chunks into a single chunk
message CompactChunks {
  // name of the database
  string db_name = 1;

  // partition key
  string partition_key = 2;

  // table name
  string table_name = 4;

  // chunk_id
  repeated uint32 chunks = 3;
}

// Split and write chunks to object store
message PersistChunks {
  // name of the database
  string db_name = 1;

  // partition key
  string partition_key = 2;

  // table name
  string table_name = 4;

  // chunk_id
  repeated uint32 chunks = 3;
}

// Drop chunk from memory and (if persisted) from object store.
message DropChunk {
  // name of the database
  string db_name = 1;

  // partition key
  string partition_key = 2;

  // table name
  string table_name = 4;

  // chunk_id
  uint32 chunk_id = 3;
}

// Drop partition from memory and (if persisted) from object store.
message DropPartition {
  // name of the database
  string db_name = 1;

  // partition key
  string partition_key = 2;

  // table name
  string table_name = 4;
}

// Wipe preserved catalog
message WipePreservedCatalog {
  // name of the database
  string db_name = 1;
}

// Soft delete data from a table on a specified predicate
// Soft delete means data is deleted from customer's point of view but
// they still physically exist in IOx storage (MUB, RUB, OS). During Query time,
// we will filter out the deleted rows.
// We will implement Hard Delete to purge deleted data.
message Delete {
  // name of the database
  string db_name = 1;

  // table name
  string table_name = 2;

  // delete predicate
  // Ideally, this can be any complicated expressions that DataFusion supports
  // but in our first version, we only support what our read buffer does which is 
  // conjunctive expressions with columns being compared to literals using = or != operators.
  // Also, to avoid user from making mistake to delete the whole table, we will force them to
  // include delete time range start and stop in different fields defined below
  string delete_predicate = 3;

  // start time range of deleting data
  string start_time = 4;

  // stop time range of deleting data
  string stop_time = 5;
}